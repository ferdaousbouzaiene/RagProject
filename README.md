# Minimal Retrieval-Augmented Generation Pipeline

Lightweight and production-ready RAG pipeline built with FastAPI, LangChain, OpenAI, and ChromaDB. It exposes a single endpoint that uses vector similarity to retrieve document context about Python, Golang and Julia and generate grounded answers for Questions about these programming languages using an LLM (GPT3.5T).


---

##  Features

- ğŸ“„ **Document Ingestion** (PDF -> chunking -> embedding)
- ğŸ” **Vector-based Retrieval** via Chroma
- ğŸ§  **Grounded LLM Generation** (OpenAI GPT-3.5T)
- ğŸ“Š **Structured Logging & Observability**
- ğŸ›¡ï¸ **Secure API Key Handling**
- ğŸ§ª **Basic Integration Tests**
- ğŸ³ **Fully Dockerized**
- ğŸ–¼ï¸  **Streamlit front-end for quick testing**

---

## Test Project

**Clone the Repository**

```bash
git clone https://github.com/ferdaousbouzaiene/repoForRagProj
cd repoForRagProj
```

** Add OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx to .env File **

** Build & Run with Docker Compose **

``` docker-compose up --build ```

API will be available at: http://localhost:8000/docs

Streamlit UI  at: http://localhost:8501


---


Project Structure:

app/

    â”œâ”€â”€ api.py            # endpoint definition
    â”œâ”€â”€ query_engine.py   # backend logic 
    â”œâ”€â”€ interface.py       # streamlit app



tests/                

    â”œâ”€â”€ test_api.py
    â”œâ”€â”€ test_retrieval.py

main.py # RAG API application entry point

ingest.py # data handling
documents/                 # Input documents

chroma_db/            # Persisted vector store


Dockerfile

docker-compose.yml

.env.example

README.md

